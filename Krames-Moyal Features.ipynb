{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13ce8ce7",
   "metadata": {},
   "source": [
    "# Improving Trajectory Classification through Kramers Moyal Coefficients\n",
    "\n",
    "### G. Viera-López, J.J. Morgado-Vega, A. Reyes, E. Altshuler, Yudivián Almeida-Cruz, Giorgio Manganini\n",
    "\n",
    "Code base to reproduce the results of the manuscript, currently under review in AI Open"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2050f3a7",
   "metadata": {},
   "source": [
    "## 1. Initialization:\n",
    "\n",
    "Make sure you are running this notebook with a python 3.9+ kernel.\n",
    "\n",
    "### 1.1 Installing software dependencies:\n",
    "\n",
    "The experimentation was conducted using the **pactus** framework for trajectory classification. In addition, we use the library **kramersmoyal** for the calculation of the Kramers Moyal Coefficients. Both can be installed from pypi by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pactus kramersmoyal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pactus** will also install **yupi**, a python library to process trajectory data.\n",
    "\n",
    "### 1.2 Importing required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116212a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import kramersmoyal\n",
    "import numpy as np\n",
    "from pactus import Dataset, featurizers\n",
    "from pactus.models import DecisionTreeModel, Evaluation, RandomForestModel, XGBoostModel\n",
    "from pactus.models.model import Model\n",
    "from yupi import Trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24c9b58",
   "metadata": {},
   "source": [
    "### 1.3 Configure variables to ensure repetitibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42d63c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0  # Fixed random seed\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f76d67d",
   "metadata": {},
   "source": [
    "## 2. Creating feature vectors from trajectories\n",
    "\n",
    "Since **pactus** has already defined a *featurizer* as a resource to allow mapping arbitrary-sized trajectories into fixed-sized feature vectors, we are going to create our own *featurizer* to use the Kramers Moyal coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d385d685",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KramersMoyalFeaturizer(featurizers.Featurizer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        bins: int = 10,\n",
    "        cutoff: int = 0,\n",
    "        bandwidth: float = 0.1,\n",
    "        powers=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.bins = np.repeat(bins, dim)\n",
    "        self.cutoff = cutoff\n",
    "        self.powers = self._all_tuples(dim) if powers is None else powers\n",
    "        self.bandwidth = bandwidth\n",
    "\n",
    "    def _all_tuples(self, size=1):\n",
    "        if size == 1:\n",
    "            return [(i,) for i in range(self.dim + 1)]\n",
    "\n",
    "        prev_tuples = self._all_tuples(size - 1)\n",
    "        tuples = []\n",
    "        for prev_t in prev_tuples:\n",
    "            tuples += [(*prev_t, i) for i in range(self.dim + 1)]\n",
    "        return tuples\n",
    "\n",
    "    def featurize(self, trajs: list[Trajectory]) -> np.ndarray:\n",
    "        feature_vector = []\n",
    "        for traj in trajs:\n",
    "            feature_vector.append(self._featurize(traj))\n",
    "        return np.array(feature_vector)\n",
    "\n",
    "    @property\n",
    "    def count(self):\n",
    "        return len(self.powers) * self.bins[0] * self.bins[1]\n",
    "\n",
    "    def _featurize(self, traj: Trajectory) -> np.ndarray:\n",
    "        kmc, _ = kramersmoyal.km(\n",
    "            traj.v, bins=self.bins, powers=self.powers, bw=self.bandwidth\n",
    "        )\n",
    "        if self.cutoff:\n",
    "            kmc = kmc[:, self.cutoff : -self.cutoff, self.cutoff : -self.cutoff]\n",
    "        return kmc.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be437919",
   "metadata": {},
   "source": [
    "Moreover, we are going to extend the more general featurizer from **pactus** to also include the features from the Kramers Moyal featurizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe3144c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniversalWithKramersMoyalFeaturizer(featurizers.CompoundFeaturizer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        bins: int = 10,\n",
    "        cutoff: int = 0,\n",
    "        bandwidth: float = 0.1,\n",
    "        powers=None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            featurizers.UniversalFeaturizer(),\n",
    "            KramersMoyalFeaturizer(\n",
    "                dim=dim, bins=bins, cutoff=cutoff, bandwidth=bandwidth, powers=powers\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b51cd3",
   "metadata": {},
   "source": [
    "In the future, we plan to extend pactus codebase to support these featurizers natively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aa0037",
   "metadata": {},
   "source": [
    "## 3. Create a function to evaluate a model on a given dataset\n",
    "\n",
    "With this function we ensure to split the dataset 70/30 for train/test respectively on every dataset, but on MNIST Stroke, where we use the original split provided (60 000 trajectories for training, 10 000 for testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e01b1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: Model, dataset: Dataset, feat_type: str):\n",
    "    # Check if the evaluation has already been done\n",
    "    file_name = f\"evaluations/{dataset.name}-{model.name}-{feat_type}-ev.json\"\n",
    "    if os.path.exists(file_name):\n",
    "        print(f\"Aleady evaluated: {file_name}\")\n",
    "        return\n",
    "\n",
    "    # Preprocess the dataset and split it into train and test sets\n",
    "    if dataset.name == \"mnist_stroke\":\n",
    "        train, test = dataset.cut(60_000)\n",
    "    else:\n",
    "        train, test = dataset.split(train_size=0.7, random_state=SEED)\n",
    "\n",
    "    # Train the model\n",
    "    model.train(data=train, cross_validation=5)\n",
    "\n",
    "    # Evaluate the model on a test dataset\n",
    "    evaluation = model.evaluate(test)\n",
    "\n",
    "    # Show the evaluation results\n",
    "    evaluation._show_general_stats()\n",
    "    evaluation.save(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686352fa",
   "metadata": {},
   "source": [
    "## 4. Generating and loading the datasets\n",
    "\n",
    "### 4.1 Diffusion dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d04901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffusion_trajs(T, dim, dt, N, gamma, sigma, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    n = int(T / dt)\n",
    "    shape = (n, dim, N)\n",
    "    v = np.zeros(shape)\n",
    "    dW = np.random.normal(scale=np.sqrt(dt), size=shape)\n",
    "\n",
    "    for i in range(n - 1):\n",
    "        v[i + 1] = v[i] - gamma(v[i]) * dt + sigma(v[i]) * dW[i]\n",
    "\n",
    "    r = v.cumsum(0) * dt\n",
    "    r -= r[0]\n",
    "    trajs = [Trajectory(points=r[..., i], dt=dt) for i in range(N)]\n",
    "    return trajs\n",
    "\n",
    "\n",
    "def DiffuisionDataset():\n",
    "    T, dim, dt, N = 50, 2, 0.005, 500\n",
    "    gamma1 = lambda v: np.array([[2], [1]]) * v\n",
    "    gamma2 = lambda v: np.array([[1], [2]]) * v\n",
    "    gamma3 = lambda v: np.array([[1], [1]]) * v\n",
    "    gamma4 = lambda v: np.array([[2], [2]]) * v\n",
    "    sigma = lambda v: np.diag([0.1, 0.1]) @ np.ones_like(v)\n",
    "    trajs1 = diffusion_trajs(T, dim, dt, N, gamma1, sigma)\n",
    "    trajs2 = diffusion_trajs(T, dim, dt, N, gamma2, sigma)\n",
    "    trajs3 = diffusion_trajs(T, dim, dt, N, gamma3, sigma)\n",
    "    trajs4 = diffusion_trajs(T, dim, dt, N, gamma4, sigma)\n",
    "\n",
    "    labels1 = [1] * len(trajs1)\n",
    "    labels2 = [2] * len(trajs2)\n",
    "    labels3 = [3] * len(trajs3)\n",
    "    labels4 = [4] * len(trajs4)\n",
    "    return Dataset(\n",
    "        \"diffusion\",\n",
    "        trajs1 + trajs2 + trajs3 + trajs4,\n",
    "        labels1 + labels2 + labels3 + labels4,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74c9d49",
   "metadata": {},
   "source": [
    "### 4.2 Data cleaning on the benchmark datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309414eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove short and static trajectories from the GeoLife dataset\n",
    "hurdat2 = Dataset.hurdat2()\n",
    "hurdat2_data = hurdat2.filter(\n",
    "    lambda t, _: len(t) > 5 and not all([v == 0 for v in t.v.reshape(-1)])\n",
    ")\n",
    "\n",
    "# Remove short and poorly time sampled trajectories and classes with few\n",
    "# trajectories from the GeoLife dataset\n",
    "geolife = Dataset.geolife()\n",
    "geolife_data = geolife.filter(lambda traj, _: len(traj) > 10 and traj.dt < 8)\n",
    "geolife_data = geolife.filter(lambda _, label: geolife_data.label_counts[label] > 5)\n",
    "\n",
    "# Remove short trajectories and classes with only one trajectory from the CMA\n",
    "# Typhoon dataset\n",
    "cma = Dataset.cma_bst()\n",
    "cma_data = cma.filter(lambda traj, label: len(traj) > 5 and cma.label_counts[label] > 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb04d27",
   "metadata": {},
   "source": [
    "### 4.3 Gathering all the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7344d89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_datset = DiffuisionDataset()\n",
    "benchmark_datasets = [\n",
    "    Dataset(geolife.name, geolife_data.trajs, geolife_data.labels),\n",
    "    Dataset.animals(),\n",
    "    Dataset(hurdat2.name, hurdat2_data.trajs, hurdat2_data.labels),\n",
    "    Dataset(cma.name, cma_data.trajs, cma_data.labels),\n",
    "    Dataset.mnist_stroke(),\n",
    "    Dataset.uci_pen_digits(),\n",
    "    Dataset.uci_characters(),\n",
    "    Dataset.uci_movement_libras(),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c667396a",
   "metadata": {},
   "source": [
    "## 5. Loading all the classification models\n",
    "\n",
    "Here we define a function to return all three classification models for a given *featurizer*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cafb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models(featurizer, num_classes: int):\n",
    "    return [\n",
    "        XGBoostModel(\n",
    "            featurizer=featurizer,\n",
    "            n_estimators=200,\n",
    "            n_jobs=6,\n",
    "            objective=\"multi:softprob\" if num_classes > 2 else \"binary:logistic\",\n",
    "            random_state=SEED,\n",
    "        ),\n",
    "        RandomForestModel(\n",
    "            featurizer=featurizer,\n",
    "            max_features=16,\n",
    "            n_estimators=200,\n",
    "            bootstrap=False,\n",
    "            warm_start=True,\n",
    "            n_jobs=6,\n",
    "            random_state=SEED,\n",
    "        ),\n",
    "        DecisionTreeModel(\n",
    "            featurizer=featurizer,\n",
    "            max_depth=7,\n",
    "            random_state=SEED,\n",
    "        ),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026809c6",
   "metadata": {},
   "source": [
    "## 6. Running all the experiments\n",
    "\n",
    "### 6.1 Evaluation of different configurations of Kramers Moyal featurizers\n",
    "\n",
    "Since the Kramers Moyal featurizers can be customized with the *bins* hyperparameter, we created a range of *bins* values to be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85712616",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_vals = list(range(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48c4df8",
   "metadata": {},
   "source": [
    "### 6.2 Evaluating all three models for every dataset and for every featurizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde311f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for bins_val in bins_vals:\n",
    "    unkm_featurizer = KramersMoyalFeaturizer(2, bins=bins_val)\n",
    "    for model in get_models(unkm_featurizer, len(diffusion_datset.classes)):\n",
    "        evaluate(model, diffusion_datset, f\"onlykm-{bins_val}\")\n",
    "\n",
    "for ds in [diffusion_datset] + benchmark_datasets:\n",
    "    # Evaluate the classical featurizer\n",
    "    univ_featurizer = featurizers.UniversalFeaturizer()\n",
    "\n",
    "    for model in get_models(univ_featurizer, len(ds.classes)):\n",
    "        evaluate(model, ds, \"univ\")\n",
    "\n",
    "    for bins_val in bins_vals:\n",
    "        unkm_featurizer = UniversalWithKramersMoyalFeaturizer(2, bins=bins_val)\n",
    "        for model in get_models(unkm_featurizer, len(ds.classes)):\n",
    "            evaluate(model, ds, f\"km-{bins_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a17388",
   "metadata": {},
   "source": [
    "### 6.3 Load evaluation files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c21f1f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "univ_evals = {}\n",
    "km_evals = {}\n",
    "onlykm_evals = {}\n",
    "\n",
    "for eval_file in Path(\"evaluations\").iterdir():\n",
    "    splitted = eval_file.name.split(\"-\")\n",
    "    ds_name, model_name, feat_type = splitted[0], splitted[1], splitted[2]\n",
    "    if feat_type == \"km\":\n",
    "        bins = int(splitted[3])\n",
    "        models_dict = km_evals.get(ds_name, {})\n",
    "        bins_dict = models_dict.get(model_name, {})\n",
    "        bins_dict[bins] = Evaluation.load(str(eval_file))\n",
    "        models_dict[model_name] = bins_dict\n",
    "        km_evals[ds_name] = models_dict\n",
    "    elif feat_type == \"onlykm\":\n",
    "        bins = int(splitted[3])\n",
    "        models_dict = onlykm_evals.get(ds_name, {})\n",
    "        bins_dict = models_dict.get(model_name, {})\n",
    "        bins_dict[bins] = Evaluation.load(str(eval_file))\n",
    "        models_dict[model_name] = bins_dict\n",
    "        onlykm_evals[ds_name] = models_dict\n",
    "    elif feat_type == \"univ\":\n",
    "        models_dict = univ_evals.get(ds_name, {})\n",
    "        models_dict[model_name] = Evaluation.load(str(eval_file))\n",
    "        univ_evals[ds_name] = models_dict\n",
    "    else:\n",
    "        raise Exception(\"Invalid eval file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78bce6f",
   "metadata": {},
   "source": [
    "### 6.4 Evaluation results of the diffusion dataset\n",
    "\n",
    "Now we analyse the test results on the accuracy and macro averaged F1 score for the *diffiusion* dataset for all the featurizers (and all bins) used in the training of the 3 models. These results are shown in Table 1 of the manuscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65eea595",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Universal Featurizer on diffusion dataset\\n\")\n",
    "print(f\"{'Model':<15} {'Acc':>10} {'F1':>10}\")\n",
    "for model, eval in univ_evals[diffusion_datset.name].items():\n",
    "    print(f\"{model:<15} {eval.acc_overall:>10.2%} {eval.f1_score:>10.2%}\")\n",
    "\n",
    "print(\"\\nOnly Kramers-Moyal Featurizer on diffusion dataset\\n\")\n",
    "for model in onlykm_evals[diffusion_datset.name]:\n",
    "    print(f\"\\nModel: {model}\\n\")\n",
    "    print(f\"{'Bins':<5} {'Acc':>10} {'F1':>10}\")\n",
    "    for bins_val, eval in sorted(\n",
    "        onlykm_evals[diffusion_datset.name][model].items(), key=lambda x: x[0]\n",
    "    ):\n",
    "        print(f\"{bins_val:<5} {eval.acc_overall:>10.2%} {eval.f1_score:>10.2%}\")\n",
    "\n",
    "print(\"\\nUniversal + Kramers-Moyal Featurizer on diffusion dataset\\n\")\n",
    "for model in km_evals[diffusion_datset.name]:\n",
    "    print(f\"\\nModel: {model}\\n\")\n",
    "    print(f\"{'Bins':<5} {'Acc':>10} {'F1':>10}\")\n",
    "    for bins_val, eval in sorted(\n",
    "        km_evals[diffusion_datset.name][model].items(), key=lambda x: x[0]\n",
    "    ):\n",
    "        print(f\"{bins_val:<5} {eval.acc_overall:>10.2%} {eval.f1_score:>10.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6710eac6",
   "metadata": {},
   "source": [
    "### 6.5 Evaluation results on the bencharmk datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ff5e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_comparing(metric_func=None) -> tuple[dict, dict]:\n",
    "    \"\"\"\n",
    "    Takes all the evaluations and a given metric and returns the value of that\n",
    "    metric using the universal featurizer and the propoused featurizer. In the\n",
    "    last case, it also stores the best bin hyperparameter value.\n",
    "    \"\"\"\n",
    "    univ_data = defaultdict(dict)\n",
    "    km_data = defaultdict(dict)\n",
    "\n",
    "    def _metric(ev):\n",
    "        return ev.acc_overall if metric_func is None else metric_func(ev)\n",
    "\n",
    "    def metric(*evals):\n",
    "        idx = 0\n",
    "        max_val = _metric(evals[0])\n",
    "        for i, ev in enumerate(evals):\n",
    "            current = _metric(ev)\n",
    "            if current > max_val:\n",
    "                idx = i\n",
    "                max_val = current\n",
    "        return idx, max_val\n",
    "\n",
    "    for ds_name, models_dict in univ_evals.items():\n",
    "        for model_name, ev in models_dict.items():\n",
    "            univ_data[ds_name][model_name] = dict(val=metric(*[ev])[1])\n",
    "\n",
    "    for ds_name, models_dict in km_evals.items():\n",
    "        for model_name, bins_dict in models_dict.items():\n",
    "            bins = list(bins_dict.keys())\n",
    "            evals = list(bins_dict.values())\n",
    "            idx, max_val = metric(*evals)\n",
    "            km_data[ds_name][model_name] = dict(best_bin=bins[idx], val=max_val)\n",
    "\n",
    "    return univ_data, km_data\n",
    "\n",
    "\n",
    "def print_results_table(univ_data, km_data):\n",
    "    \"\"\"\n",
    "    Takes two evaluation datas and prints the comparison results for each model.\n",
    "    \"\"\"\n",
    "    models_data = defaultdict(dict)\n",
    "    for ds_name, model_ev in univ_data.items():\n",
    "        for model_name, univ_ev in model_ev.items():\n",
    "            univ_val = univ_ev[\"val\"]\n",
    "            km_val = km_data[ds_name][model_name][\"val\"]\n",
    "            models_data[model_name][ds_name] = (univ_val, km_val)\n",
    "\n",
    "    for model_name, ds_data in models_data.items():\n",
    "        print(f\"\\nModel: {model_name}\\n\")\n",
    "        print(f\"{'Dataset':<25} {'Univ %':>10} {'KM %':>10}\")\n",
    "        for ds_name, data in ds_data.items():\n",
    "            univ_val = data[0] * 100\n",
    "            km_val = data[1] * 100\n",
    "            print(f\"{ds_name:<25} {univ_val:>10.2f} {km_val:>10.2f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21af47a",
   "metadata": {},
   "source": [
    "### 6.6 Get the accuracy and the macro avergaed F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50095e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "univ_acc_data, km_acc_data = get_data_for_comparing(lambda ev: ev.acc_overall)\n",
    "univ_f1_data, km_f1_data = get_data_for_comparing(lambda ev: ev.f1_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42cb7b7a",
   "metadata": {},
   "source": [
    "### 6.7 Accuracy table\n",
    "\n",
    "Show the accuracy obteined when using each feature vector for each model with all the datasets. These results are shown in Table 2 of the manuscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c2814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy\")\n",
    "print_results_table(univ_acc_data, km_acc_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "001aa14f",
   "metadata": {},
   "source": [
    "### 6.8 Macro averaged F1 score table\n",
    "\n",
    "Show the macro averaged F1 score obteined when using each feature vector for each model with all the datasets. These results are shown in table 3 of the manuscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8c120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"F1 score\")\n",
    "print_results_table(univ_f1_data, km_f1_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b5ec409f",
   "metadata": {},
   "source": [
    "### 6.9 Mean increase in accuracy and F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8311328c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_deltas(univ_data, km_data):\n",
    "    deltas = defaultdict(list)\n",
    "    for ds_name, models_ev in univ_data.items():\n",
    "        for model_name, univ_ev in models_ev.items():\n",
    "            univ_val = univ_ev[\"val\"]\n",
    "            km_val = km_data[ds_name][model_name][\"val\"]\n",
    "            deltas[model_name].append(km_val - univ_val)\n",
    "    return deltas\n",
    "\n",
    "\n",
    "delta_acc = get_model_deltas(univ_acc_data, km_acc_data)\n",
    "delta_f1 = get_model_deltas(univ_f1_data, km_f1_data)\n",
    "\n",
    "print(\"Mean increase in accuracy for each model:\")\n",
    "for model, deltas in delta_acc.items():\n",
    "    print(f\"{model:<15} {np.mean(np.array(deltas) * 100):>6.2f}%\")\n",
    "\n",
    "print(\"\\nMean increase in macro average F1 score for each model:\")\n",
    "for model, deltas in delta_f1.items():\n",
    "    print(f\"{model:<15} {np.mean(np.array(deltas) * 100):>6.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
